{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.corpus import RegexpTokenizer as regextoken\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist, bigrams, trigrams,ngrams\n",
    "from nltk import WordNetLemmatizer\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "# import gensim\n",
    "# from gensim.models.keyedvectors import KeyedVectors\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Flatten, Embedding, Conv1D, MaxPooling1D, Dropout, LSTM, GRU\n",
    "from keras.regularizers import l1, l2\n",
    "from sklearn.metrics import classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import collections\n",
    "zomato = pd.read_csv(\"zomato.csv\", na_values = [\"-\", \"\"])\n",
    "# Making a copy of the data to work on\n",
    "data = zomato.copy()\n",
    "data.drop(['url','address','menu_item','phone'],axis=1,inplace=True)\n",
    "data=data[data['rate'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['rest_type']=data['rest_type'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam=data[['rest_type','location']].groupby(['location'],as_index=False).agg(lambda x :' '.join(list(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "heybi={}\n",
    "for i in range(0,30):\n",
    "    s=sam[sam.index==i]\n",
    "    s[\"rest_type\"]= s[\"rest_type\"].apply(lambda x: x.lower())\n",
    "    tokenizer = regextoken(\"[a-zA-Z]+\") \n",
    "    review_tokens =s[\"rest_type\"].apply(tokenizer.tokenize)\n",
    "    stop = stopwords.words(\"english\")\n",
    "    stop.extend([\"rated\", \"n\", \"nan\", \"x\"])\n",
    "    review_tokens = review_tokens.apply(lambda x: [token for token in x if token not in stop])\n",
    "    all_reviews = review_tokens.astype(str).str.cat()\n",
    "    cleaned_reviews = tokenizer.tokenize(all_reviews)\n",
    "    j=s['location'].values[0]\n",
    "    heybi[j] = cleaned_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "two={}\n",
    "tri={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in heybi.keys():\n",
    "    trigram = lambda a: zip(a, a[1:],a[2:])\n",
    "    ap=list(trigram(heybi[i]))\n",
    "    trigramFreq = collections.Counter(ap)\n",
    "    tri[i]=trigramFreq.most_common(10)\n",
    "    \n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "   for i in heybi.keys():    \n",
    "    bigram = lambda a: zip(a, a[1:])\n",
    "    apw=list(bigram(heybi[i]))\n",
    "    bigramFreq = collections.Counter(apw)\n",
    "    two[i]=bigramFreq.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['BTM', 'Banashankari', 'Banaswadi', 'Bannerghatta Road', 'Basavanagudi', 'Basaveshwara Nagar', 'Bellandur', 'Bommanahalli', 'Brigade Road', 'Brookefield', 'CV Raman Nagar', 'Central Bangalore', 'Church Street', 'City Market', 'Commercial Street', 'Cunningham Road', 'Domlur', 'East Bangalore', 'Ejipura', 'Electronic City', 'Frazer Town', 'HBR Layout', 'HSR', 'Hebbal', 'Hennur', 'Hosur Road', 'ITPL Main Road, Whitefield', 'Indiranagar', 'Infantry Road', 'JP Nagar'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('delivery', 'delivery'), 13),\n",
       " (('food', 'truck'), 9),\n",
       " (('delivery', 'food'), 6),\n",
       " (('truck', 'delivery'), 5),\n",
       " (('truck', 'food'), 2),\n",
       " (('delivery', 'bakery'), 1),\n",
       " (('bakery', 'food'), 1),\n",
       " (('truck', 'takeaway'), 1),\n",
       " (('takeaway', 'delivery'), 1)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two['East Bangalore']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
